{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16e2dd-9a4c-4a2c-999a-d06e5307ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np  # For numerical operations\n",
    "from river import metrics  # For performance metrics\n",
    "from river.drift import ADWIN  # For drift detection\n",
    "from river.ensemble import BaggingClassifier  # For ensemble learning using bagging\n",
    "from river import linear_model  # For linear models\n",
    "from river.preprocessing import OneHotEncoder  # For one-hot encoding categorical variables\n",
    "import matplotlib.pyplot as plt  # For plotting visualizations\n",
    "import seaborn as sns  # For enhanced visualizations\n",
    "from sklearn.model_selection import train_test_split  # For splitting the dataset into training and testing sets\n",
    "from sklearn.linear_model import LogisticRegression  # For logistic regression model\n",
    "from sklearn.tree import DecisionTreeClassifier  # For decision tree model\n",
    "from sklearn.metrics import classification_report, roc_auc_score  # For model evaluation metrics\n",
    "from sklearn.ensemble import VotingClassifier  # For ensemble learning with voting\n",
    "from imblearn.over_sampling import SMOTE  # For handling class imbalance through oversampling\n",
    "from imblearn.under_sampling import RandomUnderSampler  # For handling class imbalance through undersampling\n",
    "from sklearn.pipeline import make_pipeline  # For creating pipelines\n",
    "import os  # For operating system functionalities\n",
    "\n",
    "# Set environment variable to limit CPU usage (optional)\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"  # Adjust as needed for parallel processing\n",
    "\n",
    "# Initialize logistic regression model with increased iteration limit\n",
    "model = LogisticRegression(max_iter=200)  # Increase from default (100)\n",
    "scaler = StandardScaler()  # Initialize scaler (not used in the pipeline yet)\n",
    "model = make_pipeline(scaler, LogisticRegression(max_iter=200))  # Create a pipeline including scaling\n",
    "model = LogisticRegression(solver='saga', max_iter=200)  # Another model initialization (overwrites previous)\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv('customer_dataset.csv')  # Read the churn dataset from a CSV file\n",
    "\n",
    "# Step 2: Check the columns in the DataFrame\n",
    "print(df.columns)  # Print column names to verify\n",
    "print(df.head())  # Display the first few rows of the DataFrame\n",
    "\n",
    "# Step 3: Create a 'date' column if necessary (optional)\n",
    "if 'date' not in df.columns and len(df) <= 1000:  # Check if 'date' column is missing and DataFrame is small\n",
    "    df['date'] = pd.date_range(start='2020-01-01', periods=len(df), freq='M')  # Generate a date range\n",
    "\n",
    "# Step 4: Identify categorical columns for one-hot encoding\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()  # Get categorical columns\n",
    "if 'churn' in categorical_cols:  # Exclude target variable from categorical columns\n",
    "    categorical_cols.remove('churn')\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)  # Convert categorical variables into dummy/indicator variables\n",
    "\n",
    "# 1. Explore for missing values\n",
    "missing_values = df.isnull().sum()  # Count missing values in each column\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_values[missing_values > 0])  # Print columns with missing values\n",
    "\n",
    "# 2. Identify anomalies using summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())  # Display summary statistics for numerical features\n",
    "\n",
    "# 3. Check for anomalies (e.g., outliers) in each column\n",
    "# Defining a function to identify anomalies based on IQR (Interquartile Range)\n",
    "def identify_anomalies(df):\n",
    "    anomalies = {}  # Initialize the anomalies dictionary\n",
    "    for column in df.select_dtypes(include=[np.number]).columns:  # Loop through numerical columns\n",
    "        Q1 = df[column].quantile(0.25)  # Calculate the first quartile\n",
    "        Q3 = df[column].quantile(0.75)  # Calculate the third quartile\n",
    "        IQR = Q3 - Q1  # Calculate Interquartile Range\n",
    "        lower_bound = Q1 - 1.5 * IQR  # Calculate lower bound for outliers\n",
    "        upper_bound = Q3 + 1.5 * IQR  # Calculate upper bound for outliers\n",
    "        anomalies[column] = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]  # Store anomalies\n",
    "\n",
    "    return anomalies  # Return dictionary of anomalies\n",
    "\n",
    "# Identify anomalies\n",
    "anomalies = identify_anomalies(df)  # Call the function to identify anomalies\n",
    "\n",
    "# Display anomalies for each numerical column\n",
    "print(\"\\nAnomalies detected in each numerical column:\")\n",
    "for column, anomaly_values in anomalies.items():\n",
    "    if not anomaly_values.empty:  # Check if there are any anomalies\n",
    "        print(f\"{column}: {anomaly_values.tolist()}\")  # Show the list of anomalies\n",
    "    else:\n",
    "        print(f\"{column}: No anomalies detected.\")  # Indicate no anomalies\n",
    "\n",
    "# 3. Visualize the distribution of key features to detect outliers\n",
    "plt.figure(figsize=(12, 6))  # Set figure size for the plot\n",
    "sns.boxplot(data=df[['age', 'income', 'monthly_bill', 'outstanding_balance']])  # Create boxplot for key features\n",
    "plt.title('Boxplot of Key Features')  # Title for the plot\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "# Optional: Visualize correlations between features\n",
    "correlation_matrix = df.corr()  # Calculate the correlation matrix\n",
    "plt.figure(figsize=(10, 8))  # Set figure size for the heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')  # Create a heatmap for correlations\n",
    "plt.title('Correlation Matrix')  # Title for the heatmap\n",
    "plt.show()  # Display the heatmap\n",
    "\n",
    "# Attempt to convert all relevant columns to numeric\n",
    "for col in df.columns:  # Loop through all columns\n",
    "    if df[col].dtype == 'object':  # Only consider object type columns\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to NaN where conversion fails\n",
    "\n",
    "# Fill NaN values with the median of each column\n",
    "df.fillna(df.median(), inplace=True)  # Replace NaN values with the median of each column\n",
    "\n",
    "# Calculate and visualize correlations again\n",
    "correlation_matrix = df.corr()  # Recalculate correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f')  # Create heatmap for correlations\n",
    "plt.title('Feature Correlation Matrix')  # Title for the heatmap\n",
    "plt.show()  # Display the heatmap\n",
    "\n",
    "# Step 5: Split the dataset into features and target variable\n",
    "X = df.drop(columns=['churn', 'date'], errors='ignore')  # Select features, drop target variable and 'date' if present\n",
    "y = df['churn']  # Select target variable\n",
    "\n",
    "# Step 6: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)  # Split dataset\n",
    "\n",
    "# Handle class imbalance using SMOTE (oversampling)\n",
    "smote = SMOTE(random_state=42)  # Initialize SMOTE\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)  # Apply SMOTE to training data\n",
    "\n",
    "# Handle class imbalance using Random Undersampling\n",
    "undersampler = RandomUnderSampler(random_state=42)  # Initialize undersampler\n",
    "X_train_undersampled, y_train_undersampled = undersampler.fit_resample(X_train, y_train)  # Apply undersampling\n",
    "\n",
    "# Train a model with SMOTE data\n",
    "model_smote = LogisticRegression(max_iter=1000)  # Initialize logistic regression model\n",
    "model_smote.fit(X_train_resampled, y_train_resampled)  # Train on resampled data\n",
    "\n",
    "# Train a model with Undersampled data\n",
    "model_undersample = LogisticRegression(max_iter=1000)  # Initialize logistic regression model\n",
    "model_undersample.fit(X_train_undersampled, y_train_undersampled)  # Train on undersampled data\n",
    "\n",
    "# Make predictions and evaluate the model with SMOTE\n",
    "y_pred_smote = model_smote.predict(X_test)  # Make predictions for the test set\n",
    "y_pred_proba_smote = model_smote.predict_proba(X_test)[:, 1]  # Get probabilities for AUC calculation\n",
    "\n",
    "# Make predictions and evaluate the model with Undersampling\n",
    "y_pred_undersample = model_undersample.predict(X_test)  # Make predictions for the test set\n",
    "y_pred_proba_undersample = model_undersample.predict_proba(X_test)[:, 1]  # Get probabilities for AUC calculation\n",
    "\n",
    "# Evaluate the models\n",
    "print(\"\\nClassification Report for SMOTE:\")\n",
    "print(classification_report(y_test, y_pred_smote))  # Print classification report for SMOTE model\n",
    "print(\"\\nAUC-ROC Score for SMOTE:\", roc_auc_score(y_test, y_pred_proba_smote))  # Print AUC score for SMOTE model\n",
    "\n",
    "print(\"\\nClassification Report for Undersampling:\")\n",
    "print(classification_report(y_test, y_pred_undersample))  # Print classification report for undersampled model\n",
    "print(\"\\nAUC-ROC Score for Undersampling:\", roc_auc_score(y_test, y_pred_proba_undersample))  # Print AUC score for undersampled model\n",
    "\n",
    "# Visualizations\n",
    "sns.countplot(x=y_train_resampled)  # Count plot for resampled training data\n",
    "plt.title('Class Distribution After SMOTE')  # Title for the plot\n",
    "plt.xlabel('Churn')  # X-axis label\n",
    "plt.ylabel('Count')  # Y-axis label\n",
    "plt.show()  # Show the plot\n",
    "\n",
    "sns.countplot(x=y_train_undersampled)  # Count plot for undersampled training data\n",
    "plt.title('Class Distribution After Undersampling')  # Title for the plot\n",
    "plt.xlabel('Churn')  # X-axis label\n",
    "plt.ylabel('Count')  # Y-axis label\n",
    "plt.show()  # Show the plot\n",
    "\n",
    "# 4. Create a 'date' column if necessary (optional)\n",
    "# Remove the following block if you don't need a date column\n",
    "# Check if the DataFrame is large before creating a date column\n",
    "if 'date' not in df.columns and len(df) <= 1000:  # Adjust threshold as needed\n",
    "    df['date'] = pd.date_range(start='2020-01-01', periods=len(df), freq='Ms')  # Generate a date range\n",
    "\n",
    "# 5. Identify categorical columns for one-hot encoding\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()  # Get categorical columns\n",
    "if 'churn' in categorical_cols:  # Exclude target variable from categorical columns\n",
    "    categorical_cols.remove('churn')\n",
    "\n",
    "# 6. One-hot encode categorical variables\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)  # Convert categorical variables into dummy/indicator variables\n",
    "\n",
    "# Initialize model and drift detector\n",
    "model = BaggingClassifier(  # Create an ensemble model using bagging\n",
    "    base_estimator=linear_model.LogisticRegression(),  # Base model for the ensemble\n",
    "    n_models=10,  # Number of models in the ensemble\n",
    "    seed=42  # Seed for reproducibility\n",
    ")\n",
    "drift_detector = ADWIN()  # Initialize the ADWIN drift detector\n",
    "metric = metrics.AUC()  # Initialize AUC metric for performance evaluation\n",
    "\n",
    "# Step 7: Train the model\n",
    "for i in range(len(X_train)):  # Loop through training data\n",
    "    model.learn_one(X_train.iloc[i].to_dict(), y_train.iloc[i])  # Train the model on one instance\n",
    "    # After learning, check for drift using the test data\n",
    "    y_pred = model.predict_one(X_test.iloc[i].to_dict())  # Make a prediction for the test instance\n",
    "    metric = metric.update(y_test.iloc[i], y_pred)  # Update the metric with the true and predicted values\n",
    "    drift_detector.update(y_pred)  # Update the drift detector with the prediction\n",
    "\n",
    "    # Check if drift is detected\n",
    "    if drift_detector.drift_detected:  # If drift is detected\n",
    "        print(f\"Drift detected at index {i}. Retraining model...\")  # Notify about drift detection\n",
    "        # Retrain the model with the latest training data\n",
    "        model = BaggingClassifier(  # Re-initialize the model\n",
    "            base_estimator=linear_model.LogisticRegression(),\n",
    "            n_models=10,\n",
    "            seed=42\n",
    "        )\n",
    "        for j in range(len(X_train)):  # Retrain on the entire training set\n",
    "            model.learn_one(X_train.iloc[j].to_dict(), y_train.iloc[j])\n",
    "        drift_detector.reset()  # Reset the drift detector after retraining\n",
    "\n",
    "# Final evaluation on the test set after training\n",
    "y_pred = model.predict(X_test.to_dict(orient='records'))  # Make predictions for the test set\n",
    "print(\"Final Classification Report:\")  # Print classification report header\n",
    "print(classification_report(y_test, y_pred))  # Display the classification report\n",
    "\n",
    "# Calculate and print final AUC-ROC score\n",
    "auc = roc_auc_score(y_test, y_pred)  # Calculate AUC-ROC score\n",
    "print(\"Final AUC-ROC Score:\", auc)  # Print the final AUC-ROC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacf13c-b81e-4adf-b543-fc051daff0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
