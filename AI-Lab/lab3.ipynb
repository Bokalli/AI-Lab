{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e8c77-4524-4576-9cb6-29358eb83a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#prompt for the author's name\n",
    "author_name = input(\"Enter your name:\")\n",
    "\n",
    "#Load the project dataset\n",
    "data = pd.read_csv('penguins.csv') #replace with your actual file path\n",
    "print(f\"\\nAuthor:{author_name}\")\n",
    "print(\"Initial Data:\")\n",
    "print(data.head())\n",
    "\n",
    "#step 2.1: Inspect for missing values\n",
    "missing_value = penguins.isnull().sum()\n",
    "\n",
    "#Display the count of missing values for each column\n",
    "mising_values[missing_values >0]\n",
    "\n",
    "#step2.2: handle missing values by dropping rows\n",
    "penguins_cleaned_dropped = penguins.dropna()\n",
    "\n",
    "#Display the shape of the original and cleaned dataset\n",
    "print(f\"Original dataset shape: {penguins.shape}\")\n",
    "print(f\"Cleaned dataset shape (dropped): {penguins_cleaned_dropped.shape}\")\n",
    "\n",
    "#step2.2: Handle missing values by imputing with the mean\n",
    "penguins_imputed = penguins.fillna(penguins.mean(numeric_only=True))\n",
    "#Display the shape of the original and cleaned dataset\n",
    "print(f\"Original dataset shape:{penguins.shape}\")\n",
    "print(f\"Cleaned dataset shape (imputed): {penguins_imputed.shape}\")\n",
    "#with the mode\n",
    "penguins_imputed_mode = penguins.fillna(penguins.mode().iloc[0])\n",
    "\n",
    "#Display the shape of the original and cleaned dataset\n",
    "print(f\"Original dataset shape:{penguins.shape}\")\n",
    "print(f\"Cleaned dataset shape (imputed with mode): {penguins_imputed_mode.shape}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#step3.1 : Create box plots to identify outliers\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "#Box plot for body mass\n",
    "plt.subplot(1,3,1)\n",
    "sns.boxplot(y=penguins['body_mass_g'])\n",
    "plt.title('Box Plot of Body Mass (g)')\n",
    "\n",
    "#Box plot for culmen Length\n",
    "plt.subplot(1,3,2)\n",
    "sns.boxplot(y=penguins['culmen_length_mm'])\n",
    "plt.title('Box Plot of Culmen Lemgth (mm)')\n",
    "\n",
    "#Box plot for culmen depth\n",
    "plt.subplot(1,3,3)\n",
    "sns.boxplot(y=penguins['culmen_depth_mm'])\n",
    "plt.title('Box Plot of Culmen Depth (mm)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#step3.2: Remove outliers based on IQR for body_mass_g, culmen_length_mm, culmen_depth_mm\n",
    "def remove_outliers(df,column):\n",
    "    Q1 =df[column].quantile(0.25)\n",
    "    Q3 =df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] >= upper_bound) ]\n",
    "\n",
    "#Start with the original dataset\n",
    "penguins_no_outliers = penguins.copy()\n",
    "\n",
    "#Remove outliers for all three columns\n",
    "for col in ['body_mass_g', 'culmen_length_mm', 'culmen_depth_mm']:\n",
    "    penguins_no_outliers = remove_outliers(penguins_no_outliers, col)\n",
    "\n",
    "#Display the shape of the original and cleaned dataset\n",
    "print(f\"Original dataset shape:{penguins.shape}\")\n",
    "print(f\"Cleaned dataset shape (no outliers): {penguins_no_outliers.shape}\")\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#step4.1: Normalize the numerical feature using min-max scaling\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#Aplly MinMax scaling to selected columns\n",
    "penguins_normalized = penguins_no_outliers.copy() #using dataset without outliers\n",
    "penguins_normalized[['body_mass_g', 'culmen_length_mm', 'culmen_depth_mm']] = Scaler.fit_transform(\n",
    "    penguins_normalized[['body_mass_g', 'culmen_length_mm', 'culmen_depth_mm']]\n",
    ")\n",
    "\n",
    "#Display the first few rows of the normalized dataset\n",
    "print(penguins_normalized[['body_mass_g', 'culmen_length_mm', 'culmen_depth_mm']].head())\n",
    "\n",
    "import pandas as pd\n",
    "#Load the penguins dataset\n",
    "penguins_df = pd.read_csv('penguins.csv')\n",
    "#Display the first few rows to verify loading\n",
    "print(penguins_df.head())\n",
    "\n",
    "#step5.1: Create a new feature based on body mass\n",
    "penguins_df['average_body_mass'] = penguins_df.groupby('species')['body_mass_g'].transform('mean')\n",
    "\n",
    "#Display the first few rows to verify new column\n",
    "print(penguins_df[['species', 'body_mass_g', 'average_body_mass']].head())\n",
    "\n",
    "#step5.2: Create a size_category based on body mass\n",
    "def categorize_size(mass):\n",
    "    if pd.isna(mass): # check for NaN\n",
    "        return 'Unkown' # Assign a category for NaN values\n",
    "    elif mass < 3500:\n",
    "        return 'Small'\n",
    "    elif 3500 <= mass < 5000:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Large'\n",
    "\n",
    "penguins_df['size_category'] = penguins_df['body_mass_g'].apply(categorize_size)\n",
    "#Display first row to verify new column\n",
    "print(penguins_df[['body_mass_g' , 'size_category']].head())\n",
    "\n",
    "\n",
    "#step6.1: Correlation analysis\n",
    "numeric_columms = penguins_df.select_dtyoes(include=['float64', 'int64']) # select only numeric columns\n",
    "correlation_matrix = numeric_colums.corr()\n",
    "\n",
    "#Display the correlation matrix\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.heatmap(correlation_matrix, annot=True, cmap= 'coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#prepare the data for modelind (drop non-numeric and target columns)\n",
    "# assuming we want to predict species based on body_mass_g, culmen_length_mm, culmen_depth_mm\n",
    "features = penguins_df[['body_mass_g','culmen_length_mm', 'culmen_depth_mm']]\n",
    "target = penguins_df['species']\n",
    "\n",
    "#train random forest model to assess feature importance\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(feature, target)\n",
    "\n",
    "#get feature importance\n",
    "importance= model.feature_importances\n",
    "\n",
    "#create a dataframe for visualization\n",
    "importance_df = pd.DatFrame({'Feature': features.columns, 'Importance': importance})\n",
    "importamce_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "#Display feature importance\n",
    "print(importance_df)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#step7.1 : split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2 , random_state=42)\n",
    "\n",
    "#display\n",
    "print(f\"Training feature set shape: {X_train.shape}\")\n",
    "print(f\"Testing feature set shape {X_test.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisiticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#example dataset creation (replace this with your actual dataset)\n",
    "x = np.array([[1,2], [np.nan, 3], [7,6], [np.nan, np.nan], [4,5]])\n",
    "y = np.array([0, 1, 0, 1, 0]) # corresponding labels\n",
    "\n",
    "#split the data into training  and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2 , random_state=42)\n",
    "\n",
    "#create an imputer to fill NaN values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "#Fit the imputer on the training data and transfrom both training and test data\n",
    "X_train_imputed = imputer.fit.transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# fit the logistic regression model\n",
    "logisitic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(X_train_imputed, y_train)\n",
    "\n",
    "#Make prediction on the test set\n",
    "y_pred = logistic_model.predict(X_test.imputed)\n",
    "\n",
    "#Print predictions\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
